{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration Globale ---\n",
    "class Config:\n",
    "    # Architecture\n",
    "    d_model: int = 256\n",
    "    n_layers: int = 4\n",
    "    n_heads: int = 4\n",
    "    block_size: int = 512\n",
    "    rms_norm_eps: float = 1e-5\n",
    "    rope_theta: float = 10000.0\n",
    "    vocab_size: int = None # Sera défini après le chargement des données\n",
    "\n",
    "    # MoE\n",
    "    num_local_experts: int = 4\n",
    "    num_experts_per_tok: int = 2\n",
    "    intermediate_size_expert_factor: int = 2 # Multiplicateur pour d_model\n",
    "    intermediate_size_shared_factor: int = 2 # Multiplicateur pour d_model\n",
    "\n",
    "    # Entraînement\n",
    "    learning_rate: float = 5e-4\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 24000\n",
    "    eval_interval: int = 300\n",
    "\n",
    "    # Dérivé / Calculé\n",
    "    d_k: int = d_model // n_heads\n",
    "    intermediate_size_expert: int = d_model * intermediate_size_expert_factor\n",
    "    intermediate_size_shared: int = d_model * intermediate_size_shared_factor\n",
    "\n",
    "    # Device\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        assert self.d_model % self.n_heads == 0, \"d_model doit être divisible par n_heads\"\n",
    "        self.d_k = self.d_model // self.n_heads\n",
    "        self.intermediate_size_expert = self.d_model * self.intermediate_size_expert_factor\n",
    "        self.intermediate_size_shared = self.d_model * self.intermediate_size_shared_factor\n",
    "\n",
    "    def print_config(self):\n",
    "        print(\"--- Configuration du Modèle ---\")\n",
    "        for key, value in self.__dict__.items():\n",
    "             # N'affiche pas les méthodes ou les attributs privés/spéciaux\n",
    "            if not key.startswith('_') and not callable(value):\n",
    "                print(f\"{key}: {value}\")\n",
    "        print(\"-----------------------------\")\n",
    "\n",
    "\n",
    "# --- Tokenizer ---\n",
    "class SimpleCharTokenizer:\n",
    "    def __init__(self, corpus):\n",
    "        self.chars = sorted(list(set(corpus)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_int = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.int_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        print(f\"Tokenizer créé avec une taille de vocabulaire de: {self.vocab_size}\")\n",
    "        # print(f\"Vocabulaire: {''.join(self.chars)}\")\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.char_to_int[ch] for ch in text if ch in self.char_to_int]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ''.join([self.int_to_char.get(id_val, '[UNK]') for id_val in ids])\n",
    "\n",
    "\n",
    "# --- Chargement et Préparation des Données IMDB ---\n",
    "def load_and_prepare_french_data(config, dataset_name=\"AIffl/Alpaca_french_mixtral\", split=\"train\", question_column=\"instruction\", answer_column=\"output\"):\n",
    "\n",
    "    try:\n",
    "        # Charger le dataset\n",
    "        dataset = load_dataset(dataset_name, split=split)\n",
    "        max_lines = 1500\n",
    "        print(f\"Dataset '{dataset_name}' chargé avec succès. taille {len(dataset)}\")\n",
    "        if len(dataset) > max_lines:\n",
    "            dataset = dataset.select(range(max_lines))\n",
    "            print(f\"Dataset réduit à {max_lines} lignes.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du dataset '{dataset_name}': {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Charger wikitext_fr pour le vocabulaire\n",
    "    wikitext_fr = load_dataset(\"asi/wikitext_fr\", split=\"train\", trust_remote_code=True)\n",
    "    corpus_raw = ' '.join(wikitext_fr['paragraph'])\n",
    "\n",
    "    tokenizer = SimpleCharTokenizer(corpus_raw)  # Initialiser le tokenizer avec wikitext_fr\n",
    "    config.vocab_size = tokenizer.vocab_size  # Mettre à jour la taille du vocabulaire dans la configuration\n",
    "    block_size = config.block_size\n",
    "    \n",
    "    # Format spécial pour les entrées/sorties (prompt engineering)\n",
    "    prompt_format = \" Question: {}\\nRéponse: {}\"\n",
    "\n",
    "    # Créer les séquences d'entraînement x et y\n",
    "    all_x = []\n",
    "    all_y = []\n",
    "    all_attention_masks = []\n",
    "\n",
    "    for idx, item in enumerate(dataset):\n",
    "        question = item[question_column]\n",
    "        answer = item[answer_column]\n",
    "        input_question = item[\"input\"]\n",
    "\n",
    "        if len(input_question) > 1:\n",
    "            answer = answer + ' \\n' + input_question\n",
    "\n",
    "        # Formater l'entrée pour qu'elle ressemble à une conversation Q/R\n",
    "        formatted_text = prompt_format.format(question, answer)\n",
    "        formatted_text = unicodedata.normalize('NFD', formatted_text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "        encoded_sequence = tokenizer.encode(formatted_text)\n",
    "        num_tokens = len(encoded_sequence)\n",
    "\n",
    "        if num_tokens > block_size:\n",
    "            # Créer des séquences chevauchantes\n",
    "            for i in range(num_tokens - block_size):\n",
    "                x_chunk = encoded_sequence[i : i + block_size]\n",
    "                y_chunk = encoded_sequence[i + 1 : i + block_size + 1]\n",
    "                attention_mask = [1] * block_size  # Masque d'attention : 1 pour les tokens réels\n",
    "                all_x.append(x_chunk)\n",
    "                all_y.append(y_chunk)\n",
    "                all_attention_masks.append(attention_mask)\n",
    "        elif num_tokens <= block_size:\n",
    "            # Ajouter du padding si la séquence est trop courte\n",
    "            padding_length = block_size - num_tokens\n",
    "            x_padded = encoded_sequence + [0] * padding_length  # Pad à droite avec des zéros\n",
    "            y_padded = encoded_sequence[1:] + [0] * (padding_length + 1) # decalage de 1 et padding\n",
    "            x_padded = x_padded[:block_size]\n",
    "            y_padded = y_padded[:block_size]\n",
    "            attention_mask = [1] * num_tokens + [0] * padding_length  # 1 pour les tokens réels, 0 pour le padding\n",
    "            all_x.append(x_padded)\n",
    "            all_y.append(y_padded)\n",
    "            all_attention_masks.append(attention_mask)\n",
    "\n",
    "    if not all_x:\n",
    "        raise ValueError(\"Aucune séquence d'entraînement n'a pu être créée. Vérifiez la taille du corpus et la taille de bloc.\")\n",
    "    \n",
    "    train_x = torch.tensor(all_x, dtype=torch.long)\n",
    "    train_y = torch.tensor(all_y, dtype=torch.long)\n",
    "    attention_mask = torch.tensor(all_attention_masks, dtype=torch.long)\n",
    "    \n",
    "    num_sequences_available = train_x.shape[0]\n",
    "    print(f\"{num_sequences_available} paires de séquences input/target créées.\")\n",
    "    print(f\"Shape de train_x: {train_x.shape}\")\n",
    "    print(f\"Shape de train_y: {train_y.shape}\")\n",
    "    print(f\"Shape de attention_mask: {attention_mask.shape}\") #afficher la shape du mask\n",
    "\n",
    "    # Ajuster la taille du batch si nécessaire\n",
    "    if num_sequences_available < config.batch_size:\n",
    "        print(f\"Attention : Nombre de séquences ({num_sequences_available}) inférieur à la taille du batch ({config.batch_size}). Ajustement de la taille du batch.\")\n",
    "        config.batch_size = num_sequences_available\n",
    "        \n",
    "    return train_x, train_y, attention_mask, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Composants du Modèle ---\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # Le gain (gamma) est un paramètre apprenable\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        # x: (..., dim)\n",
    "        # rsqrt: 1 / sqrt(x)\n",
    "        # + eps pour la stabilité numérique\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Applique la normalisation puis multiplie par le gain apprenable\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, seq_len: int, theta: float = 10000.0, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.seq_len = seq_len\n",
    "        self.theta = theta\n",
    "        self.device = device\n",
    "        self.freqs_cis = self.precompute_freqs_cis(dim, seq_len, theta).to(device)\n",
    "\n",
    "    def precompute_freqs_cis(self, dim: int, end: int, theta: float = 10000.0):\n",
    "        # Calcule les fréquences inverses: 1.0 / (theta ** (2k / dim)) pour k = 0, 1, ..., dim/2 - 1\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, dtype=torch.float)[: (dim // 2)] / dim))\n",
    "        # Crée les positions de 0 à end-1\n",
    "        t = torch.arange(end, dtype=torch.float)\n",
    "        # Calcule les angles: position * fréquence_inverse\n",
    "        freqs = torch.outer(t, freqs) # Shape (end, dim / 2)\n",
    "        # Convertit en nombres complexes: cos(angle) + i*sin(angle)\n",
    "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs) # Shape (end, dim / 2)\n",
    "        return freqs_cis\n",
    "\n",
    "    def apply_rotary_emb(self, x: torch.Tensor, freqs_cis: torch.Tensor):\n",
    "        # x: (B, T, H, Dk) ou (B, H, T, Dk) - on suppose (..., T, D) où D=dim\n",
    "        # freqs_cis: (T, Dk/2)\n",
    "        # Sépare x en parties réelles et imaginaires virtuelles\n",
    "        # x_ -> (..., T, Dk/2, 2)\n",
    "        x_ = x.float().reshape(*x.shape[:-1], -1, 2)\n",
    "        # Convertit en complexe: (..., T, Dk/2)\n",
    "        x_complex = torch.view_as_complex(x_)\n",
    "        # Adapte freqs_cis pour le broadcasting: (1, T, 1, Dk/2) ou (T, Dk/2) -> (1, T, 1, Dk/2)\n",
    "        # On suppose que freqs_cis est déjà (T, Dk/2)\n",
    "        freqs_cis = freqs_cis.unsqueeze(0).unsqueeze(2) # (1, T, 1, Dk/2)\n",
    "        # Applique la rotation complexe: x * exp(i*theta)\n",
    "        x_rotated = x_complex * freqs_cis # Broadcasting s'applique\n",
    "        # Reconvertit en réel: (..., T, Dk/2, 2)\n",
    "        x_out_real = torch.view_as_real(x_rotated)\n",
    "        # Recombine: (..., T, Dk)\n",
    "        x_out = x_out_real.flatten(3)\n",
    "        return x_out.type_as(x)\n",
    "\n",
    "    def forward(self, q: torch.Tensor, k: torch.Tensor):\n",
    "        # q, k: (B, T, H, Dk) ou (B, H, T, Dk) - on suppose (B, T, H, Dk)\n",
    "        seq_len = q.shape[1] # T\n",
    "        # Récupère les freqs précalculées pour la longueur de séquence actuelle\n",
    "        current_freqs_cis = self.freqs_cis[:seq_len].to(q.device) # (T, Dk/2)\n",
    "        # Applique RoPE\n",
    "        q_out = self.apply_rotary_emb(q, current_freqs_cis)\n",
    "        k_out = self.apply_rotary_emb(k, current_freqs_cis)\n",
    "        return q_out, k_out\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_model = config.d_model\n",
    "        self.d_k = config.d_k\n",
    "        self.head_dim = self.d_k # Alias\n",
    "\n",
    "        # Couches linéaires pour Q, K, V et la sortie\n",
    "        self.qkv_proj = nn.Linear(config.d_model, 3 * config.d_model, bias=False)\n",
    "        self.o_proj = nn.Linear(config.d_model, config.d_model, bias=False)\n",
    "\n",
    "        # RoPE (sera initialisé avec la bonne longueur de séquence plus tard si nécessaire)\n",
    "        # Note: RoPE est souvent appliqué après la projection QKV et le reshape des têtes\n",
    "        # Nous allons instancier RoPE ici mais l'appliquer dans le forward\n",
    "        self.rope = RotaryPositionalEmbedding(\n",
    "            dim=self.head_dim,\n",
    "            seq_len=config.block_size, # Longueur max\n",
    "            theta=config.rope_theta,\n",
    "            device=config.device\n",
    "        )\n",
    "\n",
    "        # Masque causal précalculé (pour ne pas le recréer à chaque fois)\n",
    "        # Shape: (1, 1, block_size, block_size)\n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.block_size, config.block_size, dtype=torch.bool)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape # Batch, Sequence Length, Embedding Dim (d_model)\n",
    "\n",
    "        # 1. Projections Q, K, V\n",
    "        qkv = self.qkv_proj(x) # (B, T, 3*C)\n",
    "\n",
    "        # 2. Séparer Q, K, V et les têtes\n",
    "        # (B, T, 3*C) -> (B, T, 3, H, Dk) -> 3 x (B, T, H, Dk)\n",
    "        qkv = qkv.view(B, T, 3, self.n_heads, self.head_dim)\n",
    "        q, k, v = qkv.chunk(3, dim=2) # Sépare sur la dimension 3 (qui a taille 3)\n",
    "        q = q.squeeze(2) # (B, T, H, Dk)\n",
    "        k = k.squeeze(2) # (B, T, H, Dk)\n",
    "        v = v.squeeze(2) # (B, T, H, Dk)\n",
    "\n",
    "        # 3. Appliquer RoPE\n",
    "        q_rope, k_rope = self.rope(q, k) # Applique RoPE sur (B, T, H, Dk)\n",
    "\n",
    "        # 4. Transposer pour le calcul de l'attention: (B, H, T, Dk)\n",
    "        q_rope = q_rope.transpose(1, 2)\n",
    "        k_rope = k_rope.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # 5. Calcul des scores d'attention (Scaled Dot-Product)\n",
    "        # (B, H, T, Dk) @ (B, H, Dk, T) -> (B, H, T, T)\n",
    "        scores = (q_rope @ k_rope.transpose(-2, -1)) * (self.head_dim ** -0.5)\n",
    "\n",
    "        # 6. Appliquer le masque causal\n",
    "        # Le masque causal a shape (1, 1, T_max, T_max)\n",
    "        # On prend la partie correspondante à la longueur T actuelle\n",
    "        mask = self.causal_mask[:, :, :T, :T]\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # 7. Appliquer Softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1) # (B, H, T, T)\n",
    "        # Gérer les NaN potentiels (si une ligne est entièrement -inf)\n",
    "        attn_weights = torch.nan_to_num(attn_weights)\n",
    "\n",
    "        # 8. Appliquer les poids d'attention à V\n",
    "        # (B, H, T, T) @ (B, H, T, Dk) -> (B, H, T, Dk)\n",
    "        attn_output = attn_weights @ v\n",
    "\n",
    "        # 9. Concaténer les têtes\n",
    "        # (B, H, T, Dk) -> (B, T, H, Dk) -> (B, T, C)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # 10. Projection de sortie\n",
    "        output = self.o_proj(attn_output)\n",
    "        return output\n",
    "\n",
    "class ExpertMLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        hidden_dim = config.intermediate_size_expert\n",
    "        # Combine les projections Gate et Up pour l'efficacité (comme dans Llama)\n",
    "        self.gate_up_proj = nn.Linear(config.d_model, 2 * hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_dim, config.d_model, bias=False)\n",
    "        self.act_fn = nn.SiLU() # SwiGLU implicite\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (..., d_model)\n",
    "        gate_up = self.gate_up_proj(x) # (..., 2 * hidden_dim)\n",
    "        gate, up = gate_up.chunk(2, dim=-1) # (..., hidden_dim)\n",
    "        fused_activation = self.act_fn(gate) * up # (..., hidden_dim)\n",
    "        output = self.down_proj(fused_activation) # (..., d_model)\n",
    "        return output\n",
    "\n",
    "class MoE(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.num_experts = config.num_local_experts\n",
    "        self.num_experts_per_tok = config.num_experts_per_tok\n",
    "        self.d_model = config.d_model\n",
    "\n",
    "        # La couche de routage (gate)\n",
    "        self.gate = nn.Linear(config.d_model, self.num_experts, bias=False)\n",
    "\n",
    "        # Les experts eux-mêmes\n",
    "        self.experts = nn.ModuleList([ExpertMLP(config) for _ in range(self.num_experts)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B, T, C = x.shape # Batch, Sequence Length, Dim\n",
    "\n",
    "        # 1. Obtenir les logits du routeur\n",
    "        # x: (B, T, C) -> router_logits: (B, T, num_experts)\n",
    "        router_logits = self.gate(x)\n",
    "\n",
    "        # 2. Sélectionner les Top-K experts et calculer les poids de routage\n",
    "        # routing_weights: (B, T, k), selected_experts: (B, T, k)\n",
    "        routing_weights, selected_experts = torch.topk(\n",
    "            router_logits, self.num_experts_per_tok, dim=-1\n",
    "        )\n",
    "        # Appliquer Softmax (ou Sigmoid comme dans le code original) sur les poids des k experts sélectionnés\n",
    "        # Le code original utilise Sigmoid, ce qui ne garantit pas que la somme soit 1.\n",
    "        # Softmax semble plus standard pour pondérer les sorties. Utilisons Softmax ici.\n",
    "        routing_weights = F.softmax(routing_weights, dim=-1, dtype=torch.float).to(x.dtype)\n",
    "        # Si on veut Sigmoid comme l'original:\n",
    "        # routing_weights = torch.sigmoid(routing_weights)\n",
    "\n",
    "        # 3. Préparer pour le calcul parallèle des experts\n",
    "        # Aplatir B et T pour traiter chaque token indépendamment\n",
    "        x_flat = x.view(-1, C) # (B*T, C)\n",
    "        routing_weights_flat = routing_weights.view(-1, self.num_experts_per_tok) # (B*T, k)\n",
    "        selected_experts_flat = selected_experts.view(-1, self.num_experts_per_tok) # (B*T, k)\n",
    "\n",
    "        # Initialiser le tenseur de sortie final (même taille que x_flat)\n",
    "        final_output_flat = torch.zeros_like(x_flat)\n",
    "\n",
    "        # Obtenir les indices des tokens (0 à B*T - 1)\n",
    "        token_indices = torch.arange(B * T, device=x.device)\n",
    "\n",
    "        # 4. Itérer sur les experts et calculer les sorties\n",
    "        # C'est l'approche la plus simple, mais peut être inefficace.\n",
    "        # L'approche du code original avec scatter_add est plus performante.\n",
    "        # Répliquons l'approche scatter_add :\n",
    "\n",
    "        # Créer un index plat pour les tokens et les experts sélectionnés\n",
    "        # token_idx_expanded: (B*T*k) - Répète chaque index de token k fois\n",
    "        token_idx_expanded = token_indices.repeat_interleave(self.num_experts_per_tok)\n",
    "        # expert_idx_flat: (B*T*k) - Les indices des experts sélectionnés pour chaque token*expert\n",
    "        expert_idx_flat = selected_experts_flat.view(-1)\n",
    "        # routing_weights_flat_expanded: (B*T*k) - Les poids associés\n",
    "        routing_weights_flat_expanded = routing_weights_flat.view(-1)\n",
    "\n",
    "        # Créer un batch pour chaque expert\n",
    "        # expert_inputs: (B*T*k, C) - Les inputs x_flat dupliqués pour chaque expert sélectionné\n",
    "        expert_inputs = x_flat[token_idx_expanded]\n",
    "\n",
    "        # Calculer les sorties des experts de manière batchée (si possible, sinon itérer)\n",
    "        # Ici, nous allons utiliser une boucle pour la clarté, mais une implémentation\n",
    "        # optimisée utiliserait des opérations batchées ou scatter/gather.\n",
    "        expert_outputs_list = []\n",
    "        for i in range(self.num_experts):\n",
    "            # Trouver les indices des tokens qui ont sélectionné cet expert\n",
    "            mask = (expert_idx_flat == i)\n",
    "            if mask.any():\n",
    "                # Sélectionner les inputs et les poids pour cet expert\n",
    "                current_inputs = expert_inputs[mask] # (N_i, C) où N_i est le nb de tokens routés vers l'expert i\n",
    "                # Calculer la sortie de l'expert\n",
    "                current_outputs = self.experts[i](current_inputs) # (N_i, C)\n",
    "                # Pondérer par les poids de routage correspondants\n",
    "                current_weights = routing_weights_flat_expanded[mask].unsqueeze(1) # (N_i, 1)\n",
    "                weighted_outputs = current_outputs * current_weights # (N_i, C)\n",
    "\n",
    "                # Utiliser scatter_add_ pour ajouter les sorties pondérées aux bonnes positions\n",
    "                # Indices originaux des tokens qui ont utilisé cet expert\n",
    "                original_token_indices = token_idx_expanded[mask]\n",
    "                final_output_flat.scatter_add_(0, original_token_indices.unsqueeze(1).expand(-1, C), weighted_outputs)\n",
    "\n",
    "        # 5. Remettre en forme la sortie\n",
    "        # final_output_flat: (B*T, C) -> (B, T, C)\n",
    "        final_output = final_output_flat.view(B, T, C)\n",
    "\n",
    "        return final_output\n",
    "\n",
    "class SharedMLP(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        hidden_dim = config.intermediate_size_shared\n",
    "        # Structure similaire à ExpertMLP mais avec des dimensions potentiellement différentes\n",
    "        self.gate_proj = nn.Linear(config.d_model, hidden_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.d_model, hidden_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(hidden_dim, config.d_model, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate = self.gate_proj(x)\n",
    "        up = self.up_proj(x)\n",
    "        fused_activation = self.act_fn(gate) * up\n",
    "        output = self.down_proj(fused_activation)\n",
    "        return output\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_model = config.d_model\n",
    "\n",
    "        # Normalisation avant l'attention\n",
    "        self.input_layernorm = RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        # Module d'attention\n",
    "        self.self_attn = Attention(config)\n",
    "        # Normalisation avant le MoE/MLP\n",
    "        self.post_attention_layernorm = RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "        # Mixture of Experts\n",
    "        self.moe = MoE(config)\n",
    "        # MLP Partagé\n",
    "        self.shared_mlp = SharedMLP(config)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Connexion résiduelle 1 (autour de l'attention)\n",
    "        residual = x\n",
    "        h = self.input_layernorm(x)\n",
    "        attn_output = self.self_attn(h)\n",
    "        h = residual + attn_output\n",
    "\n",
    "        # Connexion résiduelle 2 (autour du MoE et MLP partagé)\n",
    "        residual = h\n",
    "        h_norm = self.post_attention_layernorm(h)\n",
    "        # Calculer les sorties du MoE et du MLP partagé\n",
    "        moe_output = self.moe(h_norm)\n",
    "        shared_output = self.shared_mlp(h_norm)\n",
    "        # Combiner les sorties (addition simple comme dans le papier Llama 3/4)\n",
    "        h = residual + moe_output + shared_output\n",
    "\n",
    "        return h\n",
    "\n",
    "# --- Modèle Llama Complet ---\n",
    "\n",
    "class LlamaMoEModel(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Embedding des tokens\n",
    "        self.tok_embeddings = nn.Embedding(config.vocab_size, config.d_model)\n",
    "\n",
    "        # Blocs Transformer\n",
    "        self.layers = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "\n",
    "        # Normalisation finale\n",
    "        self.norm = RMSNorm(config.d_model, eps=config.rms_norm_eps)\n",
    "\n",
    "        # Tête de classification (prédit le prochain token)\n",
    "        # Note: Parfois, les poids de l'embedding sont liés (partagés) avec cette couche\n",
    "        self.output = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialisation des poids (optionnel mais recommandé)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Initialisation standard pour les transformeurs\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        B, T = tokens.shape\n",
    "        # 1. Obtenir les embeddings des tokens\n",
    "        h = self.tok_embeddings(tokens) # (B, T, C)\n",
    "\n",
    "        # 2. Passer à travers les blocs Transformer\n",
    "        for layer in self.layers:\n",
    "            h = layer(h)\n",
    "\n",
    "        # 3. Normalisation finale\n",
    "        h = self.norm(h)\n",
    "\n",
    "        # 4. Calculer les logits de sortie\n",
    "        logits = self.output(h) # (B, T, VocabSize)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, tokenizer, prompt: str, max_new_tokens: int, block_size: int, device: str):\n",
    "        self.eval() # Mettre le modèle en mode évaluation\n",
    "        # Encoder l'amorce\n",
    "        prompt_ids = tokenizer.encode(prompt)\n",
    "        context = torch.tensor([prompt_ids], dtype=torch.long, device=device)\n",
    "\n",
    "        generated_ids = []\n",
    "\n",
    "        print(f\"Génération à partir de : '{prompt}'\")\n",
    "        with torch.no_grad(): # Pas besoin de calculer les gradients\n",
    "            for _ in range(max_new_tokens):\n",
    "                # S'assurer que le contexte ne dépasse pas block_size\n",
    "                context_cond = context[:, -block_size:]\n",
    "                # Obtenir les logits du modèle\n",
    "                logits = self(context_cond) # (B, T, VocabSize)\n",
    "                # Prendre les logits du dernier token seulement\n",
    "                logits_last = logits[:, -1, :] # (B, VocabSize)\n",
    "                # Appliquer softmax pour obtenir les probabilités\n",
    "                probs = F.softmax(logits_last, dim=-1) # (B, VocabSize)\n",
    "                # Échantillonner le prochain token basé sur les probabilités\n",
    "                next_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "                # Ajouter le token généré à la séquence\n",
    "                context = torch.cat((context, next_token), dim=1)\n",
    "                generated_ids.append(next_token.item())\n",
    "                # Afficher le caractère généré (optionnel)\n",
    "                # print(tokenizer.decode([next_token.item()]), end='', flush=True)\n",
    "\n",
    "\n",
    "        print(\"\\n--- Génération terminée ---\")\n",
    "        self.train() # Remettre en mode entraînement par défaut\n",
    "        return tokenizer.decode(prompt_ids + generated_ids)\n",
    "\n",
    "# --- Fonctions d'Entraînement ---\n",
    "def get_batch(data_x, data_y, attention_mask, batch_size, device):\n",
    "    num_sequences = data_x.shape[0]\n",
    "    indices = torch.randint(0, num_sequences, (batch_size,))\n",
    "    xb = data_x[indices].to(device)\n",
    "    yb = data_y[indices].to(device)\n",
    "    attention_mask_batch = attention_mask[indices].to(device) # Récupérer aussi le masque d'attention pour le batch\n",
    "    return xb, yb, attention_mask_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_x, train_y, attention_mask, config, optimizer, criterion):\n",
    "    print(f\"\\n--- Démarrage de l'entraînement pour {config.epochs} époques ---\")\n",
    "    losses = []\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        xb, yb, attention_mask_batch = get_batch(train_x, train_y, attention_mask, config.batch_size, config.device)\n",
    "        logits = model(xb)\n",
    "\n",
    "        B_loss, T_loss, V_loss = logits.shape\n",
    "        logits_for_loss = logits.view(B_loss * T_loss, V_loss)\n",
    "        targets_for_loss = yb.view(B_loss * T_loss)\n",
    "        # Appliquer le masque d'attention pour ne considérer que les tokens non-padding dans le calcul de la perte.\n",
    "        loss = criterion(logits_for_loss, targets_for_loss)\n",
    "        loss = (loss * attention_mask_batch.view(-1)).sum() / attention_mask_batch.sum() #IMPORTANT\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_loss = loss.item()\n",
    "        losses.append(current_loss)\n",
    "        if epoch % config.eval_interval == 0 or epoch == config.epochs - 1:\n",
    "            print(f\"  Époque {epoch+1}/{config.epochs}, Perte: {current_loss:.4f}\")\n",
    "\n",
    "    print(\"--- Entraînement terminé ---\")\n",
    "\n",
    "    # Afficher le graphique des pertes\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(losses)\n",
    "        plt.title(\"Perte d'entraînement au fil des époques\")\n",
    "        plt.xlabel(\"Époque\")\n",
    "        plt.ylabel(\"Perte\")\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"Matplotlib non trouvé, graphique des pertes ignoré.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Impossible d'afficher le graphique des pertes: {e}\")\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-07T08:51:21.812Z",
     "iopub.execute_input": "2025-05-07T08:48:35.535736Z",
     "iopub.status.busy": "2025-05-07T08:48:35.535099Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    config = Config()\n",
    "    config.__post_init__()\n",
    "    train_x, train_y, attention_mask, tokenizer = load_and_prepare_french_data(config)\n",
    "\n",
    "    # Charger le modèle\n",
    "    model_path = \"../llama_moe_model_final.pt\"\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model = LlamaMoEModel(config).to(config.device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_model(model, train_x, train_y, attention_mask, config, optimizer, criterion)\n",
    "\n",
    "    # Sauvegarder le modèle fine-tuned (important)\n",
    "    torch.save(model.state_dict(), \"llama_moe_imdb_finetuned.pt\")\n",
    "    print(\"Modèle fine-tuned sauvegardé!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-07T08:46:47.333472Z",
     "iopub.status.busy": "2025-05-07T08:46:47.333124Z",
     "iopub.status.idle": "2025-05-07T08:46:48.133541Z",
     "shell.execute_reply": "2025-05-07T08:46:48.132735Z",
     "shell.execute_reply.started": "2025-05-07T08:46:47.333454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_prompt = \"Question: Quel est le produit de 6 et 2?\\nReponse:\"\n",
    "test_generated_text = model.generate(\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=test_prompt,\n",
    "    max_new_tokens=50,\n",
    "    block_size=config.block_size, \n",
    "    device=config.device\n",
    ")\n",
    "print(\"\\nTexte Généré :\")\n",
    "print(test_generated_text)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 324109,
     "modelInstanceId": 303626,
     "sourceId": 380070,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
